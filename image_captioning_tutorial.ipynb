{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Tutorial Using the CNN-LSTM Encoder-Decoder Model ### \n",
    "\n",
    "\n",
    "### Motivation ###\n",
    "As humans, we find it really easy to describe an image that we see, so a natural question to ask is if we can teach a computer to describe an image that we give it. In order to achieve this, we will be using a **CNN-LSTM Encoder Decoder Model**. \n",
    "\n",
    "### Understanding the CNN-LSTM Encoder Decoder Model ###\n",
    "\n",
    "In order to teach a computer to describe an image that we give, we need to use a fairly complex deep learning model. This deep learning model is called a CNN-LSTM Encoder Decoder Model. It consists of 2 major components that do the following:\n",
    "    1. A Convolutional Neural Network (CNN) is a type of deep learning algorithm that, given an image, is able to give specific importance to certain features of the image (\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "# small library for seeing the progress of loops.\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import re\n",
    "TOKEN_FILE = 'C:\\\\Users\\\\Andrey\\\\Desktop\\\\CS-390-Neural-Image-Processing\\\\Final Project\\\\text\\\\Flickr8k.token.txt'\n",
    "TEXT_DATASET = 'C:\\\\Users\\\\Andrey\\\\Desktop\\\\CS-390-Neural-Image-Processing\\\\Final Project\\\\text'\n",
    "IMAGE_DATASET ='C:\\\\Users\\\\Andrey\\\\Desktop\\\\CS-390-Neural-Image-Processing\\\\Final Project\\\\images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting feature vector for all images\n",
    "\n",
    "def extract_features(img_directory):\n",
    "    model = InceptionV3(include_top = False, pooling='avg')\n",
    "    img_files = os.listdir(img_directory)\n",
    "    print(len(img_files))\n",
    "    features = dict()\n",
    "    for img in tqdm(img_files):\n",
    "        img_file = img_directory + '/' + img # specify the exact image\n",
    "        image = Image.open(img_file) # open it \n",
    "        image = image.resize((299,299)) # resize it to be an image of 299, 299\n",
    "        \n",
    "        image = np.expand_dims(image, axis=0) / 255 # add an extra dimension and convert pixel RGB values to decimals\n",
    "        \n",
    "        feature = model.predict(image)\n",
    "        features[img] = feature\n",
    "    \n",
    "    return features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8fa149bb8a40028a27ad08b5c2953a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = extract_features(IMAGE_DATASET)\n",
    "dump(features, open('features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "# opens the file and returns it.\n",
    "def load_doc(filename):\n",
    "    with open(filename) as fp:\n",
    "        text = fp.read()\n",
    "        fp.close()\n",
    "        return text\n",
    "\n",
    "\n",
    "def all_img_captions(filename):\n",
    "    image_captions = load_doc(TOKEN_FILE)\n",
    "    #print(image_captions)\n",
    "    image_captions = image_captions.split('\\n')\n",
    "    \n",
    "    img_captions_dict = defaultdict(list)\n",
    "    for img_caption in image_captions[:-1]:\n",
    "        img_caption = img_caption.strip() # remove new line\n",
    "        img, caption = img_caption.split('\\t') # split on tab\n",
    "        # print(img_caption)\n",
    "        img = img[:-2] # remove last 2 characters in the string\n",
    "        \n",
    "        img_captions_dict[img].append(caption)\n",
    "    \n",
    "    return img_captions_dict\n",
    "\n",
    "def clean_text(img_caption_dict):\n",
    "    for img, captions in img_caption_dict.items():\n",
    "        for i, caption in enumerate(captions):\n",
    "            caption = caption.lower() # lower case the words\n",
    "            caption = re.sub('[^A-Za-z0-9]+', ' ', caption) # removes all characters that are not a letter or number\n",
    "            caption = [word for word in caption.split() if (len(word) > 1 and word.isalpha())] # only include words of length 1 or greater, and alphabetic in nature.\n",
    "            captions[i] = ' '.join(caption) # convert back to string\n",
    "        \n",
    "        img_caption_dict[img] = captions\n",
    "    \n",
    "    return img_caption_dict\n",
    "    \n",
    "def text_vocabulary(img_captions_dict):\n",
    "    vocab_list = list()\n",
    "    for captions in img_captions_dict.values(): # take all captions\n",
    "        for caption in captions:\n",
    "            #print(caption)\n",
    "            caption = caption.split() # # make string into a word list\n",
    "            vocab_list.extend(caption) # add all words into vocab_list\n",
    "        \n",
    "    vocab_list = list(set(vocab_list)) # remove duplicates\n",
    "\n",
    "    return vocab_list # return list\n",
    "\n",
    "def save_captions(img_captions_dict, filename):\n",
    "    with open(filename, 'a') as fp:\n",
    "        lines = []\n",
    "        for img, captions in img_captions_dict.items():\n",
    "            for caption in captions:\n",
    "                line = img + '\\t' + caption + '\\n'\n",
    "                lines.append(line)\n",
    "        lines[-1] = lines[-1].strip() # remove new line from last string\n",
    "        for line in lines:\n",
    "            fp.write(line)\n",
    "        \n",
    "        \n",
    "img_captions_dict = all_img_captions(TOKEN_FILE)\n",
    "img_captions_dict = clean_text(img_captions_dict)\n",
    "\n",
    "vocabulary = text_vocabulary(img_captions_dict)\n",
    "save_captions(img_captions_dict, 'img_caption.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open('features.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset for training the model\n",
    "def load_images(filename): \n",
    "    file = load_doc(filename) # open file\n",
    "    imgs = file.split('\\n')[:-1] # convert to list and ignore last newline char\n",
    "    return imgs # return\n",
    "\n",
    "def load_clean_img_caption(filename, images):\n",
    "    subset_img_caption_dict = defaultdict(list) # create a dictionary containing lists as values\n",
    "    file = load_doc(filename).split('\\n') # load file given to us and split on newline\n",
    "    for line in file:\n",
    "        \n",
    "        \n",
    "        line = line.split('\\t') # remove tabs\n",
    "        if len(line) <= 1: # remove any empty lines \n",
    "            continue\n",
    "        \n",
    "        img, caption = line\n",
    "        if img in images: # if image is in our set\n",
    "            caption = '<start> ' + caption + ' <end>' # append identifiers\n",
    "            subset_img_caption_dict[img].append(caption) # append to captions list\n",
    "\n",
    "    return subset_img_caption_dict # return\n",
    "\n",
    "def load_subset_features(images, all_features):\n",
    "    select_features = {} # dictionary\n",
    "    for image in images:\n",
    "        select_features[image] = all_features[image] # set value to be feature vector corresponding to image in list\n",
    "    return select_features # return\n",
    "\n",
    "filename = TEXT_DATASET +'\\\\Flickr_8k.trainImages.txt'\n",
    "train_imgs = load_images(filename)\n",
    "\n",
    "train_img_captions = load_clean_img_caption('img_caption.txt', train_imgs)\n",
    "train_features = load_subset_features(train_imgs, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7317\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "\n",
    "def dict_to_list(img_caption_dict):\n",
    "    captions_list = []\n",
    "    for captions in img_caption_dict.values():\n",
    "        for caption in captions:\n",
    "            captions_list.append(caption) # converts our dictionary into a list containing just the captions\n",
    "    \n",
    "    return captions_list \n",
    "\n",
    "\n",
    "def tokenize(captions_list):\n",
    "    \n",
    "    tokenizer = Tokenizer() # using keras tokenizer\n",
    "    tokenizer.fit_on_texts(captions_list) # update internal vocabulary based on our list of captions\n",
    "    return tokenizer # return the tokenizer\n",
    "\n",
    "def max_length(captions_list):\n",
    "    \n",
    "    return max(len(caption.split()) for caption in captions_list) # return the length of the longest caption\n",
    "\n",
    "\n",
    "captions_list = dict_to_list(train_img_captions)\n",
    "tokenizer = tokenize(captions_list)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb')) # dumps our tokenizer into a file called tokenizer.p\n",
    "vocab_size = len(tokenizer.word_index) + 1 # computes our total vocab size. Word index is a dictionary mapping the word to the number of occurences of that word. By taking the length of the dictionary and adding 1, we get the overall vocabulary length.\n",
    "print(vocab_size)\n",
    "max_length = max_length(captions_list)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 42, 3, 87, 170, 6, 116, 52, 387, 11, 394, 3, 27, 4415, 625, 1]\n",
      "15\n",
      "[2, 18, 313, 64, 195, 119, 1]\n",
      "21\n",
      "[2, 39, 18, 116, 64, 195, 2062, 1]\n",
      "28\n",
      "[2, 39, 18, 116, 4, 394, 19, 60, 2062, 1]\n",
      "37\n",
      "[2, 39, 18, 3, 87, 170, 313, 64, 195, 2901, 1]\n",
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((47, 2048), (47, 35), (47, 7317))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_generator(img_captions_dict, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for img, captions in img_captions_dict.items():\n",
    "            feature = features[img][0] # get corresponding extracted image feature vector\n",
    "            #print(feature)\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, captions, feature) \n",
    "            yield [input_image, input_sequence], output_word # generates current batch of data\n",
    "\n",
    "def create_sequences(captions, max_length, tokenizer, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    \n",
    "    for caption in captions:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0] # takes each word in the text and replaces it with word_index frequency\n",
    "        print(seq)\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i] # creates the x2 (in_seq) and y (out_seq) values. x2 value is a list that contains all words up and not including current word. y is the word to predict\n",
    "            \n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0] # standardizes the length of the list for each in seq. This is done in order for our model to work.\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0] # converts our output array, which contains a number, to a specific categorical value. This value is dependent on the size of our vocabulary.\n",
    "            \n",
    "            # append to our dataset\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "        \n",
    "        # print(len(X1))\n",
    "    return np.array(X1), np.array(X2), np.array(y) # return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_captioning_model(vocab_size, max_length):\n",
    "    imageInput = Input(shape=(2048,)) # Image Feature Vector input\n",
    "    fe1 = Dropout(.5)(imageInput) # dropout layer\n",
    "    fe2 = Dense(256, activation='relu')(fe1) # First Fully connected layer that uses a RELU activation Function\n",
    "    \n",
    "    #LSTM sequence model\n",
    "    textInput = Input(shape=(max_length,)) # caption input\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(textInput) # convert our caption input into a word embedding of input size 7317, output size 256, input length 33\n",
    "    # The mask_zero parameter is a boolean paramter that is used to mask out an input value of 0. Useful for RNN Layers that take variable input (for example in our case)\n",
    "    se2 = Dropout(.5)(se1) # Dropout layer to avoid overfitting\n",
    "    se3 = LSTM(256)(se2) # LSTM Layer used to learn the word Embedding and output a layer with 256 neurons\n",
    "    \n",
    "    # Merge both of the models\n",
    "    decoder1 = add([fe2,se3]) # combines the 2 functions together\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1) # adds another dense layer for learning the combined function\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2) # output layer containing 7317 outputs, determined by the probability assigned by softmax\n",
    "    \n",
    "    img_cap_model = Model(inputs=[imageInput, textInput], outputs=outputs) # Define the model with the inputs and outputs discussed\n",
    "    img_cap_model.compile(loss='categorical_crossentropy', optimizer='adam') # compile model and give it a loss of categoriacal crossentropy and use the adam optimzer\n",
    "    \n",
    "    print(img_cap_model.summary()) # prints summary of how the model looks like \n",
    "    plot_model(img_cap_model, to_file='model.png', show_shapes=True) # used to obtain graphical image of the model\n",
    "    \n",
    "    return img_cap_model # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 33, 256)      1873152     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 2048)         0           input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 33, 256)      0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 256)          524544      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 256)          525312      dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 256)          0           dense_21[0][0]                   \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 256)          65792       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 7317)         1880469     dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,869,269\n",
      "Trainable params: 4,869,269\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "6000/6000 [==============================] - 771s 128ms/step - loss: 4.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 792s 132ms/step - loss: 3.7235\n",
      "6000/6000 [==============================] - 788s 131ms/step - loss: 3.4474\n",
      "6000/6000 [==============================] - 791s 132ms/step - loss: 3.2847\n",
      "6000/6000 [==============================] - 791s 132ms/step - loss: 3.1736\n",
      "6000/6000 [==============================] - 782s 130ms/step - loss: 3.0873\n",
      "6000/6000 [==============================] - 788s 131ms/step - loss: 3.0227\n",
      "6000/6000 [==============================] - 805s 134ms/step - loss: 2.9711\n",
      "6000/6000 [==============================] - 802s 134ms/step - loss: 2.9276\n",
      "6000/6000 [==============================] - 839s 140ms/step - loss: 2.8912\n"
     ]
    }
   ],
   "source": [
    "model = image_captioning_model(vocab_size, max_length) # call model\n",
    "epochs=10 # initialize number of epochs\n",
    "steps=len(train_img_captions) # number of train image captions there are (6000)\n",
    "os.mkdir('models') # save the models\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_img_captions, train_features, tokenizer, max_length) # generate the dataset\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1) # function used to allow us to learn the model\n",
    "    model.save('models/model_'+ str(i) + '.h5') # save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
